# Copyright 2025 Cisco Systems, Inc. and its affiliates
#
# SPDX-License-Identifier: Apache-2.0
# TODO(https://github.com/RobustIntelligence/faith/issues/197): this benchmark
#   contains questions with multiple correct answers.
#   This is not supported by the MCQA benchmark format.
#
# Current set of answers: 'B' 'A' 'AB' 'AD' 'CD' 'BD' 'AC' 'ABC' 'ACD' 'BCD' 'D' 'ABD' 'C' 'BC', 'ABCD' 'AA'
---
benchmark: !from
  '$BENCHMARKS_ROOT/mcqa-template.yaml["mcqa_common_benchmark"]':
    metadata: !from
      '$BENCHMARKS_ROOT/base-template.yaml["metadata"]':
        name: "seceval"
        description: |-
          SecEval is a benchmark designed to evaluate the performance of
          AI models in the field of computer security. This subset contains
          ~1,000 multiple-choice questions, each with four answer choices and
          one correct answer labeled as 'A', 'B', 'C', or 'D'.
        urls:
          - https://xuanwuai.github.io/SecEval/
          - https://github.com/XuanwuAI/SecEval
        categories: ["security"]
    source:
      huggingface: !from
        '$BENCHMARKS_ROOT/sources-template.yaml["sources"]["huggingface"]':
          path: "XuanwuAI/SecEval"
          test_split: "train"
      options:
        dataframe_transform_expr: |
          df.assign(
              choices=[
                  [choice_str.split(": ")[-1] for choice_str in choice_lst]
                  for choice_lst in df["choices"].tolist()
              ],
          )[
              # Remove rows with empty strings or None values.
              ~df.isin(["", None]).any(axis=1) &
              df["answer"].isin(["A", "B", "C", "D"])
          ][["question", "choices", "answer"]]
